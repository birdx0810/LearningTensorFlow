# Img2Text

COCO Image Dataset

## Image Caption

1. Show and Tell
2. Show, Attend and Tell
3. Show, Observe and Tell: attribute-driven attention model for image captioning. IJCAI 2018. [paper](https://www.ijcai.org/Proceedings/2018/84)
4. Show, Reward, and Tell: Adversarial Visual Story Generation. AAAI 2018/ACM 2019. [paper](https://dl.acm.org/doi/10.1145/3291925)
5. Show, Control and Tell: A Framework for Generating Controllable and Grounded Captions. CVPR 2019. [paper](https://arxiv.org/abs/1811.10652)

   \*\*\*\*

## Visual QA

1. Ask, Attend and Answer: Exploring Question-Guided Spatial Attention for Visual Question Answering. CVPR 2016. [paper](https://arxiv.org/abs/1511.05234)
2. Show, Ask, Attend, and Answer: A Strong Baseline For Visual Question Answering. 2017. ****[paper](https://arxiv.org/abs/1704.03162)

## Image-Text Representations

1. ViLBERT. [paper](https://arxiv.org/abs/1908.02265)
2. VisualBERT. [paper](https://arxiv.org/abs/1908.03557)
3. LXMERT. [paper](https://arxiv.org/abs/1908.07490)
4. VLBERT. [paper](https://arxiv.org/abs/1908.08530)
5. UNITER: UNiversal Image-TExt Representation Learning. Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, Jingjing Liu. ECCV 2020. [paper](https://arxiv.org/abs/1909.11740)
6. OSCAR: Object-Semantics Aligned Pre-training for Vision-Language Tasks. Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, Yejin Choi, Jianfeng Gao. ECCV 2020. [paper](https://arxiv.org/abs/2004.06165)



